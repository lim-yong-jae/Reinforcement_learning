# Summary
When a parameter space has a certain underlying structure, the ordinary gradient of a function does not represent its steepest direction, but the natural gradient does. Natural gradient is that it considers parameter space's base vector for steepest direction. For example, let's consider that manifold 
<p align="center"> <img src="./img/manifold.png" alt="MLE" width="20%" height="20%"/> </p>

In that data manifold, 'x1' and 'x2' is close, when its distance is calculated by Euclidean space's metric function. But considering data structure, its distance can be calculated from "red line". So we should consider data structure and, redefine its metric function.


# Results


# Reference
* A Natural policy gradient paper: https://repository.upenn.edu/cgi/viewcontent.cgi?article=1128&context=statistics_papers  
* Natural Gradient Works Efficiently in Learning paper: https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.452.7280&rep=rep1&type=pdf  
* image from https://parkgeonyeong.github.io/Manifold-Learning-%EC%95%8C%EA%B3%A0%EB%A6%AC%EC%A6%98-(SNE,-/.)/
* 
