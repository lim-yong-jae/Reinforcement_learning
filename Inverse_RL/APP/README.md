# Summary
## Abstract  
We can get reward function set which is the result of solving IRL problem, but it is difficult to choose one.   

### Precondition:  
 We think of the expert as trying to maximize a reward function that is expressible as **a linear combination of known features**. 

### Result:  
 We show that our algorithm terminates in a small number of iterations, and that **even though we may never recover the expert’s reward function,** the policy output by the algorithm will attain performance close to that of the expert, where **here performance is measured with respect to the expert’s unknown reward function.** 
 
### conclusion:  
 **It will show that their algorithm choose plausible reward function which expert uses by comparing other reward funtion in reward function set.**  



## Introduction  



# Reference
* Apprenticeship Learning via Inverse Reinforcement Learning: http://people.eecs.berkeley.edu/~russell/classes/cs294/s11/readings/Abbeel+Ng:2004.pdf    
* 
